{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxY8XhOTic1v"
   },
   "source": [
    "# FastGRNN in Keras\n",
    "\n",
    "This is modified to Keras implementation of FastGRNN from a original simple notebook that illustrates the usage of tensorflow implementation. This is just only training result. The USPS dataset is used as with the original code.  \n",
    "Original code is as follows.  \n",
    "https://github.com/microsoft/EdgeML/tree/master/examples/tf/FastCells\n",
    "  \n",
    "Please refer to `fetch_usps.py` and run it for downloading and cleaning up the dataset.  \n",
    "\n",
    "The original code license is as follows.  \n",
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9vjLLaUic1w"
   },
   "outputs": [],
   "source": [
    "import helpermethods\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, Input, RNN\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#Provide the GPU number to be used\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] =''\n",
    "\n",
    "#FastRNN and FastGRNN imports\n",
    "from rnn import FastGRNNCellKeras\n",
    "\n",
    "# Fixing seeds for reproducibility\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqqbmZRSic10"
   },
   "source": [
    "# USPS Data\n",
    "\n",
    "--- Here is completely copied from the original code. ---  \n",
    "  \n",
    "  \n",
    "It is assumed that the USPS data has already been downloaded and processed with [fetch_usps.py](fetch_usps.py) and [process_usps.py](process_usps.py), and is present in the `./usps10` subdirectory.\n",
    "\n",
    "Note: Even though usps10 is not a time-series dataset, it can be assumed as, a time-series where each row is coming in at one single time.\n",
    "So the number of timesteps = 16 and inputDims = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2460,
     "status": "ok",
     "timestamp": 1590377616205,
     "user": {
      "displayName": "kosodate ikumen",
      "photoUrl": "",
      "userId": "00910566635899833198"
     },
     "user_tz": -540
    },
    "id": "dAYSt4ksic10",
    "outputId": "64136db3-dbd2-42b8-8830-fd21a6d88792"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Dimension:  256\n",
      "Num classes:  10\n"
     ]
    }
   ],
   "source": [
    "#Loading and Pre-processing dataset for FastCells\n",
    "dataDir = \"usps10\"\n",
    "(dataDimension, numClasses, Xtrain, Ytrain, Xtest, Ytest, mean, std) = helpermethods.preProcessData(dataDir)\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ep6IfUEFic14"
   },
   "source": [
    "# Model Parameters\n",
    "\n",
    "---Here is completely copied from the original code.---  \n",
    "  \n",
    "\n",
    "FastRNN and FastGRNN work for most of the hyper-parameters with which you could acheive decent accuracies on LSTM/GRU. Over and above that, you can use low-rank, sparsity and quatization to reduce model size upto 45x when compared to LSTM/GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xhodKCAZic15"
   },
   "outputs": [],
   "source": [
    "cell = \"FastGRNN\" # Choose between FastGRNN, FastRNN, UGRNN, GRU and LSTM\n",
    "\n",
    "inputDims = 16 #features taken in by RNN in one timestep\n",
    "hiddenDims = 32 #hidden state of RNN\n",
    "\n",
    "totalEpochs = 300\n",
    "batchSize = 100\n",
    "\n",
    "learningRate = 0.01\n",
    "decayStep = 200\n",
    "decayRate = 0.1\n",
    "\n",
    "outFile = None #provide your file, if you need all the logging info in a file\n",
    "\n",
    "#low-rank parameterisation for weight matrices. None => Full Rank\n",
    "wRank = None \n",
    "uRank = None \n",
    "\n",
    "#Sparsity of the weight matrices. x => 100*x % are non-zeros\n",
    "sW = 1.0 \n",
    "sU = 1.0\n",
    "\n",
    "#Non-linearities for the RNN architecture. Can choose from \"tanh, sigmoid, relu, quantTanh, quantSigm\"\n",
    "update_non_linearity = \"tanh\"\n",
    "gate_non_linearity = \"sigmoid\"\n",
    "\n",
    "assert dataDimension % inputDims == 0, \"Infeasible per step input, Timesteps have to be integer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFh0Dq_nic2B"
   },
   "source": [
    "# FastCell Graph Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnORCgL4ic2B"
   },
   "outputs": [],
   "source": [
    "#Create appropriate RNN cell object based on choice\n",
    "if cell == \"FastGRNN\":\n",
    "    FastCell = FastGRNNCellKeras(hiddenDims, gate_non_linearity=gate_non_linearity,\n",
    "                            update_non_linearity=update_non_linearity,\n",
    "                            wRank=wRank, uRank=uRank)\n",
    "    \n",
    "###!!!Other Cells is not implemented for Keras!!!!###\n",
    "elif cell == \"FastRNN\":\n",
    "    FastCell = FastRNNCell(hiddenDims, update_non_linearity=update_non_linearity,\n",
    "                           wRank=wRank, uRank=uRank)\n",
    "elif cell == \"UGRNN\":\n",
    "    FastCell = UGRNNLRCell(hiddenDims, update_non_linearity=update_non_linearity,\n",
    "                           wRank=wRank, uRank=uRank)\n",
    "elif cell == \"GRU\":\n",
    "    FastCell = GRULRCell(hiddenDims, update_non_linearity=update_non_linearity,\n",
    "                         wRank=wRank, uRank=uRank)\n",
    "elif cell == \"LSTM\":\n",
    "    FastCell = LSTMLRCell(hiddenDims, update_non_linearity=update_non_linearity,\n",
    "                          wRank=wRank, uRank=uRank)\n",
    "else:\n",
    "    sys.exit('Exiting: No Such Cell as ' + cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lowRJLOTic2E"
   },
   "source": [
    "# FastCell Trainer Model for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 778,
     "status": "ok",
     "timestamp": 1590379229636,
     "user": {
      "displayName": "kosodate ikumen",
      "photoUrl": "",
      "userId": "00910566635899833198"
     },
     "user_tz": -540
    },
    "id": "TMp3-gTeic2F",
    "outputId": "33c7bf34-b1f4-4b6d-a027-2fca6772ff20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 16, 16)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 16, 16)            0         \n",
      "_________________________________________________________________\n",
      "rnn (RNN)                    (None, 32)                3204      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 1,932\n",
      "Trainable params: 1,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = inputs = Input(shape=[int(dataDimension / inputDims), inputDims], name='input')\n",
    "layer_shape = K.int_shape(x)#x.get_shape()\n",
    "print(layer_shape)\n",
    "x = RNN(FastCell, return_sequences=False, name='rnn')(x)\n",
    "out = Dense(numClasses, activation='softmax', name='dense')(x)\n",
    "model = Model(inputs=inputs, outputs=out)\n",
    "Adam_ = Adam(lr=0.01)\n",
    "model.compile(optimizer=Adam_, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "#Y = tf.placeholder(\"float\", [None, numClasses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZoVLV6qic2H"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 152480,
     "status": "ok",
     "timestamp": 1590379385156,
     "user": {
      "displayName": "kosodate ikumen",
      "photoUrl": "",
      "userId": "00910566635899833198"
     },
     "user_tz": -540
    },
    "id": "20OfXNzsic2H",
    "outputId": "663f7d60-8668-4fdb-e575-405180a1a7c1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7291 samples, validate on 2007 samples\n",
      "Epoch 1/300\n",
      "7291/7291 [==============================] - 1s 167us/sample - loss: 1.3699 - acc: 0.5186 - val_loss: 1.0842 - val_acc: 0.6413\n",
      "Epoch 2/300\n",
      "7291/7291 [==============================] - 1s 105us/sample - loss: 0.8093 - acc: 0.7254 - val_loss: 0.8386 - val_acc: 0.7235\n",
      "Epoch 3/300\n",
      "7291/7291 [==============================] - 1s 98us/sample - loss: 0.6489 - acc: 0.7855 - val_loss: 0.7413 - val_acc: 0.7534\n",
      "Epoch 4/300\n",
      "7291/7291 [==============================] - 1s 92us/sample - loss: 0.5442 - acc: 0.8238 - val_loss: 0.6908 - val_acc: 0.7778\n",
      "Epoch 5/300\n",
      "7291/7291 [==============================] - 1s 91us/sample - loss: 0.4598 - acc: 0.8526 - val_loss: 0.6257 - val_acc: 0.8037\n",
      "Epoch 6/300\n",
      "7291/7291 [==============================] - 1s 91us/sample - loss: 0.3864 - acc: 0.8745 - val_loss: 0.5689 - val_acc: 0.8286\n",
      "Epoch 7/300\n",
      "7291/7291 [==============================] - 1s 96us/sample - loss: 0.3374 - acc: 0.8940 - val_loss: 0.5384 - val_acc: 0.8416\n",
      "Epoch 8/300\n",
      "7291/7291 [==============================] - 1s 88us/sample - loss: 0.3072 - acc: 0.9015 - val_loss: 0.5033 - val_acc: 0.8505\n",
      "Epoch 9/300\n",
      "7291/7291 [==============================] - 1s 86us/sample - loss: 0.2719 - acc: 0.9126 - val_loss: 0.4717 - val_acc: 0.8620\n",
      "Epoch 10/300\n",
      "7291/7291 [==============================] - 1s 95us/sample - loss: 0.2458 - acc: 0.9214 - val_loss: 0.4658 - val_acc: 0.8675\n",
      "Epoch 11/300\n",
      "7291/7291 [==============================] - 1s 92us/sample - loss: 0.2296 - acc: 0.9294 - val_loss: 0.4815 - val_acc: 0.8680\n",
      "Epoch 12/300\n",
      "7291/7291 [==============================] - 1s 116us/sample - loss: 0.2130 - acc: 0.9328 - val_loss: 0.4396 - val_acc: 0.8729\n",
      "Epoch 13/300\n",
      "7291/7291 [==============================] - 1s 130us/sample - loss: 0.1972 - acc: 0.9369 - val_loss: 0.4567 - val_acc: 0.8729\n",
      "Epoch 14/300\n",
      "7291/7291 [==============================] - 1s 103us/sample - loss: 0.1870 - acc: 0.9395 - val_loss: 0.4584 - val_acc: 0.8789\n",
      "Epoch 15/300\n",
      "7291/7291 [==============================] - 1s 120us/sample - loss: 0.1783 - acc: 0.9431 - val_loss: 0.4428 - val_acc: 0.8824\n",
      "Epoch 16/300\n",
      "7291/7291 [==============================] - 1s 128us/sample - loss: 0.1643 - acc: 0.9475 - val_loss: 0.4547 - val_acc: 0.8759\n",
      "Epoch 17/300\n",
      "7291/7291 [==============================] - 1s 116us/sample - loss: 0.1613 - acc: 0.9494 - val_loss: 0.4236 - val_acc: 0.8959\n",
      "Epoch 18/300\n",
      "7291/7291 [==============================] - 1s 116us/sample - loss: 0.1489 - acc: 0.9520 - val_loss: 0.4664 - val_acc: 0.8724\n",
      "Epoch 19/300\n",
      "7291/7291 [==============================] - 1s 138us/sample - loss: 0.1549 - acc: 0.9493 - val_loss: 0.4457 - val_acc: 0.8819\n",
      "Epoch 20/300\n",
      "7291/7291 [==============================] - 1s 119us/sample - loss: 0.1433 - acc: 0.9534 - val_loss: 0.4584 - val_acc: 0.8784\n",
      "Epoch 21/300\n",
      "7291/7291 [==============================] - 1s 108us/sample - loss: 0.1437 - acc: 0.9539 - val_loss: 0.4646 - val_acc: 0.8789\n",
      "Epoch 22/300\n",
      "7291/7291 [==============================] - 1s 114us/sample - loss: 0.1452 - acc: 0.9547 - val_loss: 0.4587 - val_acc: 0.8789\n",
      "Epoch 23/300\n",
      "7291/7291 [==============================] - 1s 110us/sample - loss: 0.1500 - acc: 0.9510 - val_loss: 0.4501 - val_acc: 0.8854\n",
      "Epoch 24/300\n",
      "7291/7291 [==============================] - 1s 105us/sample - loss: 0.1387 - acc: 0.9569 - val_loss: 0.4493 - val_acc: 0.8849\n",
      "Epoch 25/300\n",
      "7291/7291 [==============================] - 1s 124us/sample - loss: 0.1343 - acc: 0.9582 - val_loss: 0.4215 - val_acc: 0.8904\n",
      "Epoch 26/300\n",
      "7291/7291 [==============================] - 1s 115us/sample - loss: 0.1247 - acc: 0.9610 - val_loss: 0.4214 - val_acc: 0.8939\n",
      "Epoch 27/300\n",
      "7291/7291 [==============================] - 1s 111us/sample - loss: 0.1178 - acc: 0.9616 - val_loss: 0.4013 - val_acc: 0.8994\n",
      "Epoch 28/300\n",
      "7291/7291 [==============================] - 1s 115us/sample - loss: 0.0972 - acc: 0.9696 - val_loss: 0.4266 - val_acc: 0.8954\n",
      "Epoch 29/300\n",
      "7291/7291 [==============================] - 1s 116us/sample - loss: 0.1069 - acc: 0.9652 - val_loss: 0.4035 - val_acc: 0.9043\n",
      "Epoch 30/300\n",
      "7291/7291 [==============================] - 1s 130us/sample - loss: 0.0949 - acc: 0.9704 - val_loss: 0.4434 - val_acc: 0.8904\n",
      "Epoch 31/300\n",
      "7291/7291 [==============================] - 1s 143us/sample - loss: 0.1034 - acc: 0.9642 - val_loss: 0.4373 - val_acc: 0.8939\n",
      "Epoch 32/300\n",
      "7291/7291 [==============================] - 1s 124us/sample - loss: 0.1143 - acc: 0.9634 - val_loss: 0.4279 - val_acc: 0.8979\n",
      "Epoch 33/300\n",
      "7291/7291 [==============================] - 1s 123us/sample - loss: 0.1066 - acc: 0.9650 - val_loss: 0.4343 - val_acc: 0.8824\n",
      "Epoch 34/300\n",
      "7291/7291 [==============================] - 1s 112us/sample - loss: 0.1152 - acc: 0.9634 - val_loss: 0.4476 - val_acc: 0.8934\n",
      "Epoch 35/300\n",
      "7291/7291 [==============================] - 1s 125us/sample - loss: 0.1078 - acc: 0.9653 - val_loss: 0.4366 - val_acc: 0.9008\n",
      "Epoch 36/300\n",
      "7291/7291 [==============================] - 1s 120us/sample - loss: 0.0858 - acc: 0.9739 - val_loss: 0.4355 - val_acc: 0.9008\n",
      "Epoch 37/300\n",
      "7291/7291 [==============================] - 1s 116us/sample - loss: 0.1023 - acc: 0.9658 - val_loss: 0.4452 - val_acc: 0.8989\n",
      "Epoch 38/300\n",
      "7291/7291 [==============================] - 1s 111us/sample - loss: 0.0851 - acc: 0.9713 - val_loss: 0.4448 - val_acc: 0.9008\n",
      "Epoch 39/300\n",
      "7291/7291 [==============================] - 1s 133us/sample - loss: 0.0936 - acc: 0.9701 - val_loss: 0.4427 - val_acc: 0.9043\n",
      "Epoch 40/300\n",
      "7291/7291 [==============================] - 1s 170us/sample - loss: 0.1028 - acc: 0.9674 - val_loss: 0.4064 - val_acc: 0.8979\n",
      "Epoch 41/300\n",
      "7291/7291 [==============================] - 1s 112us/sample - loss: 0.0965 - acc: 0.9689 - val_loss: 0.3959 - val_acc: 0.9063\n",
      "Epoch 42/300\n",
      "7291/7291 [==============================] - 1s 102us/sample - loss: 0.0774 - acc: 0.9763 - val_loss: 0.4197 - val_acc: 0.9058\n",
      "Epoch 43/300\n",
      "7291/7291 [==============================] - 1s 99us/sample - loss: 0.1027 - acc: 0.9674 - val_loss: 0.4116 - val_acc: 0.9033\n",
      "Epoch 44/300\n",
      "7291/7291 [==============================] - 1s 114us/sample - loss: 0.0872 - acc: 0.9715 - val_loss: 0.4151 - val_acc: 0.9088\n",
      "Epoch 45/300\n",
      "7291/7291 [==============================] - 1s 119us/sample - loss: 0.0826 - acc: 0.9738 - val_loss: 0.4273 - val_acc: 0.9003\n",
      "Epoch 46/300\n",
      "7291/7291 [==============================] - 1s 105us/sample - loss: 0.0920 - acc: 0.9706 - val_loss: 0.4554 - val_acc: 0.8869\n",
      "Epoch 47/300\n",
      "7291/7291 [==============================] - 1s 103us/sample - loss: 0.0841 - acc: 0.9744 - val_loss: 0.3854 - val_acc: 0.8999\n",
      "Epoch 48/300\n",
      "7291/7291 [==============================] - 1s 102us/sample - loss: 0.0822 - acc: 0.9733 - val_loss: 0.4090 - val_acc: 0.9078\n",
      "Epoch 49/300\n",
      "7291/7291 [==============================] - 1s 98us/sample - loss: 0.0892 - acc: 0.9697 - val_loss: 0.4299 - val_acc: 0.9018\n",
      "Epoch 50/300\n",
      "7291/7291 [==============================] - 1s 104us/sample - loss: 0.0694 - acc: 0.9771 - val_loss: 0.4228 - val_acc: 0.9023\n",
      "Epoch 51/300\n",
      "7291/7291 [==============================] - 1s 109us/sample - loss: 0.0746 - acc: 0.9764 - val_loss: 0.4291 - val_acc: 0.9018\n",
      "Epoch 52/300\n",
      "7291/7291 [==============================] - 1s 105us/sample - loss: 0.0687 - acc: 0.9778 - val_loss: 0.3797 - val_acc: 0.8999\n",
      "Epoch 53/300\n",
      "7291/7291 [==============================] - 1s 112us/sample - loss: 0.0778 - acc: 0.9750 - val_loss: 0.3999 - val_acc: 0.9058\n",
      "Epoch 54/300\n",
      "7291/7291 [==============================] - 1s 100us/sample - loss: 0.0739 - acc: 0.9752 - val_loss: 0.4030 - val_acc: 0.9123\n",
      "Epoch 55/300\n",
      "7291/7291 [==============================] - 1s 120us/sample - loss: 0.0659 - acc: 0.9785 - val_loss: 0.4178 - val_acc: 0.9083\n",
      "Epoch 56/300\n",
      "7291/7291 [==============================] - 1s 109us/sample - loss: 0.0641 - acc: 0.9793 - val_loss: 0.4005 - val_acc: 0.9153\n",
      "Epoch 57/300\n",
      "7291/7291 [==============================] - 1s 135us/sample - loss: 0.0773 - acc: 0.9748 - val_loss: 0.4273 - val_acc: 0.9058\n",
      "Epoch 58/300\n",
      "7291/7291 [==============================] - 1s 136us/sample - loss: 0.0659 - acc: 0.9792 - val_loss: 0.4146 - val_acc: 0.9058\n",
      "Epoch 59/300\n",
      "7291/7291 [==============================] - 1s 168us/sample - loss: 0.0701 - acc: 0.9783 - val_loss: 0.4194 - val_acc: 0.9033\n",
      "Epoch 60/300\n",
      "7291/7291 [==============================] - 1s 123us/sample - loss: 0.0947 - acc: 0.9698 - val_loss: 0.4096 - val_acc: 0.9018\n",
      "Epoch 61/300\n",
      "7291/7291 [==============================] - 1s 116us/sample - loss: 0.0683 - acc: 0.9767 - val_loss: 0.4408 - val_acc: 0.9033\n",
      "Epoch 62/300\n",
      "7291/7291 [==============================] - 1s 114us/sample - loss: 0.0731 - acc: 0.9767 - val_loss: 0.4222 - val_acc: 0.9073\n",
      "Epoch 63/300\n",
      "7291/7291 [==============================] - 1s 130us/sample - loss: 0.0541 - acc: 0.9829 - val_loss: 0.4320 - val_acc: 0.9068\n",
      "Epoch 64/300\n",
      "7291/7291 [==============================] - 1s 126us/sample - loss: 0.0432 - acc: 0.9874 - val_loss: 0.4032 - val_acc: 0.9143\n",
      "Epoch 65/300\n",
      "7291/7291 [==============================] - 1s 110us/sample - loss: 0.0524 - acc: 0.9837 - val_loss: 0.4273 - val_acc: 0.9073\n",
      "Epoch 66/300\n",
      "7291/7291 [==============================] - 1s 140us/sample - loss: 0.0616 - acc: 0.9785 - val_loss: 0.4444 - val_acc: 0.9098\n",
      "Epoch 67/300\n",
      "7291/7291 [==============================] - 1s 122us/sample - loss: 0.0557 - acc: 0.9823 - val_loss: 0.4545 - val_acc: 0.9048\n",
      "Epoch 68/300\n",
      "7291/7291 [==============================] - 1s 106us/sample - loss: 0.0632 - acc: 0.9793 - val_loss: 0.4475 - val_acc: 0.9018\n",
      "Epoch 69/300\n",
      "7291/7291 [==============================] - 1s 116us/sample - loss: 0.0549 - acc: 0.9809 - val_loss: 0.4020 - val_acc: 0.9133\n",
      "Epoch 70/300\n",
      "7291/7291 [==============================] - 1s 142us/sample - loss: 0.0460 - acc: 0.9837 - val_loss: 0.4467 - val_acc: 0.9118\n",
      "Epoch 71/300\n",
      "7291/7291 [==============================] - 1s 172us/sample - loss: 0.1115 - acc: 0.9648 - val_loss: 0.4527 - val_acc: 0.8939\n",
      "Epoch 72/300\n",
      "7291/7291 [==============================] - 1s 141us/sample - loss: 0.1225 - acc: 0.9616 - val_loss: 0.3894 - val_acc: 0.9123\n",
      "Epoch 73/300\n",
      "7291/7291 [==============================] - 1s 144us/sample - loss: 0.0865 - acc: 0.9719 - val_loss: 0.3471 - val_acc: 0.9133\n",
      "Epoch 74/300\n",
      "7291/7291 [==============================] - 1s 180us/sample - loss: 0.0740 - acc: 0.9759 - val_loss: 0.3915 - val_acc: 0.9128\n",
      "Epoch 75/300\n",
      "7291/7291 [==============================] - 1s 137us/sample - loss: 0.0543 - acc: 0.9824 - val_loss: 0.4125 - val_acc: 0.9113\n",
      "Epoch 76/300\n",
      "7291/7291 [==============================] - 2s 241us/sample - loss: 0.0613 - acc: 0.9807 - val_loss: 0.4257 - val_acc: 0.9123\n",
      "Epoch 77/300\n",
      "7291/7291 [==============================] - 1s 129us/sample - loss: 0.0578 - acc: 0.9804 - val_loss: 0.4157 - val_acc: 0.9128\n",
      "Epoch 78/300\n",
      "7291/7291 [==============================] - 1s 107us/sample - loss: 0.0575 - acc: 0.9809 - val_loss: 0.3991 - val_acc: 0.9163\n",
      "Epoch 79/300\n",
      "7291/7291 [==============================] - 1s 102us/sample - loss: 0.0637 - acc: 0.9798 - val_loss: 0.3952 - val_acc: 0.9133\n",
      "Epoch 80/300\n",
      "7291/7291 [==============================] - 1s 114us/sample - loss: 0.0541 - acc: 0.9823 - val_loss: 0.4056 - val_acc: 0.9158\n",
      "Epoch 81/300\n",
      "7291/7291 [==============================] - 1s 105us/sample - loss: 0.0508 - acc: 0.9838 - val_loss: 0.4408 - val_acc: 0.9143\n",
      "Epoch 82/300\n",
      "7291/7291 [==============================] - 1s 105us/sample - loss: 0.0517 - acc: 0.9824 - val_loss: 0.4075 - val_acc: 0.9128\n",
      "Epoch 83/300\n",
      "7291/7291 [==============================] - 1s 89us/sample - loss: 0.0704 - acc: 0.9756 - val_loss: 0.4045 - val_acc: 0.9103\n",
      "Epoch 84/300\n",
      "7291/7291 [==============================] - 1s 130us/sample - loss: 0.0638 - acc: 0.9794 - val_loss: 0.3947 - val_acc: 0.9128\n",
      "Epoch 85/300\n",
      "7291/7291 [==============================] - 1s 114us/sample - loss: 0.0602 - acc: 0.9815 - val_loss: 0.3970 - val_acc: 0.9128\n",
      "Epoch 86/300\n",
      "7291/7291 [==============================] - 1s 116us/sample - loss: 0.0504 - acc: 0.9846 - val_loss: 0.4129 - val_acc: 0.9168\n",
      "Epoch 87/300\n",
      "7291/7291 [==============================] - 1s 114us/sample - loss: 0.0390 - acc: 0.9868 - val_loss: 0.4259 - val_acc: 0.9103\n",
      "Epoch 88/300\n",
      "7291/7291 [==============================] - 1s 103us/sample - loss: 0.0381 - acc: 0.9889 - val_loss: 0.4154 - val_acc: 0.9158\n",
      "Epoch 89/300\n",
      "7291/7291 [==============================] - 1s 101us/sample - loss: 0.0333 - acc: 0.9905 - val_loss: 0.3970 - val_acc: 0.9233\n",
      "Epoch 90/300\n",
      "7291/7291 [==============================] - 1s 101us/sample - loss: 0.0314 - acc: 0.9912 - val_loss: 0.4833 - val_acc: 0.9108\n",
      "Epoch 91/300\n",
      "7291/7291 [==============================] - 1s 116us/sample - loss: 0.0638 - acc: 0.9790 - val_loss: 0.3734 - val_acc: 0.9178\n",
      "Epoch 92/300\n",
      "7291/7291 [==============================] - 1s 123us/sample - loss: 0.0571 - acc: 0.9808 - val_loss: 0.4144 - val_acc: 0.9118\n",
      "Epoch 93/300\n",
      "7291/7291 [==============================] - 1s 182us/sample - loss: 0.0489 - acc: 0.9842 - val_loss: 0.3962 - val_acc: 0.9173\n",
      "Epoch 94/300\n",
      "7291/7291 [==============================] - 1s 123us/sample - loss: 0.0450 - acc: 0.9882 - val_loss: 0.4224 - val_acc: 0.9173\n",
      "Epoch 95/300\n",
      "7291/7291 [==============================] - 1s 134us/sample - loss: 0.0395 - acc: 0.9877 - val_loss: 0.4099 - val_acc: 0.9163\n",
      "Epoch 96/300\n",
      "7291/7291 [==============================] - 1s 128us/sample - loss: 0.0422 - acc: 0.9861 - val_loss: 0.5419 - val_acc: 0.9008\n",
      "Epoch 97/300\n",
      "7291/7291 [==============================] - 1s 120us/sample - loss: 0.1702 - acc: 0.9494 - val_loss: 0.4657 - val_acc: 0.9003\n",
      "Epoch 98/300\n",
      "7291/7291 [==============================] - 1s 108us/sample - loss: 0.0692 - acc: 0.9765 - val_loss: 0.4153 - val_acc: 0.9118\n",
      "Epoch 99/300\n",
      "7291/7291 [==============================] - 1s 117us/sample - loss: 0.0447 - acc: 0.9846 - val_loss: 0.3994 - val_acc: 0.9193\n",
      "Epoch 100/300\n",
      "7291/7291 [==============================] - 1s 125us/sample - loss: 0.0379 - acc: 0.9892 - val_loss: 0.4221 - val_acc: 0.9098\n",
      "Epoch 101/300\n",
      "7291/7291 [==============================] - 1s 115us/sample - loss: 0.0419 - acc: 0.9882 - val_loss: 0.4442 - val_acc: 0.9158\n",
      "Epoch 102/300\n",
      "7291/7291 [==============================] - 1s 102us/sample - loss: 0.0524 - acc: 0.9816 - val_loss: 0.4215 - val_acc: 0.9108\n",
      "Epoch 103/300\n",
      "7291/7291 [==============================] - 1s 105us/sample - loss: 0.0487 - acc: 0.9851 - val_loss: 0.4813 - val_acc: 0.9063\n",
      "Epoch 104/300\n",
      "7291/7291 [==============================] - 1s 124us/sample - loss: 0.0455 - acc: 0.9845 - val_loss: 0.4488 - val_acc: 0.9088\n",
      "Epoch 105/300\n",
      "7291/7291 [==============================] - 1s 113us/sample - loss: 0.0538 - acc: 0.9819 - val_loss: 0.4229 - val_acc: 0.9193\n",
      "Epoch 106/300\n",
      "7291/7291 [==============================] - 1s 99us/sample - loss: 0.0419 - acc: 0.9851 - val_loss: 0.4680 - val_acc: 0.9158\n",
      "Epoch 107/300\n",
      "7291/7291 [==============================] - 1s 93us/sample - loss: 0.0442 - acc: 0.9868 - val_loss: 0.4362 - val_acc: 0.9198\n",
      "Epoch 108/300\n",
      "7291/7291 [==============================] - 1s 88us/sample - loss: 0.0340 - acc: 0.9893 - val_loss: 0.4466 - val_acc: 0.9118\n",
      "Epoch 109/300\n",
      "7291/7291 [==============================] - 1s 85us/sample - loss: 0.0342 - acc: 0.9877 - val_loss: 0.4543 - val_acc: 0.9168\n",
      "Epoch 110/300\n",
      "7291/7291 [==============================] - 1s 86us/sample - loss: 0.0457 - acc: 0.9856 - val_loss: 0.4808 - val_acc: 0.9123\n",
      "Epoch 111/300\n",
      "7291/7291 [==============================] - 1s 85us/sample - loss: 0.0773 - acc: 0.9752 - val_loss: 0.4271 - val_acc: 0.9098\n",
      "Epoch 112/300\n",
      "7291/7291 [==============================] - 1s 85us/sample - loss: 0.0504 - acc: 0.9827 - val_loss: 0.4830 - val_acc: 0.9053\n",
      "Epoch 113/300\n",
      "7291/7291 [==============================] - 1s 95us/sample - loss: 0.0550 - acc: 0.9808 - val_loss: 0.4672 - val_acc: 0.9028\n",
      "Epoch 114/300\n",
      "7291/7291 [==============================] - 1s 83us/sample - loss: 0.0570 - acc: 0.9816 - val_loss: 0.4199 - val_acc: 0.9193\n",
      "Epoch 115/300\n",
      "7291/7291 [==============================] - 1s 83us/sample - loss: 0.0366 - acc: 0.9893 - val_loss: 0.4263 - val_acc: 0.9153\n",
      "Epoch 116/300\n",
      "7291/7291 [==============================] - 1s 77us/sample - loss: 0.0332 - acc: 0.9886 - val_loss: 0.4742 - val_acc: 0.9128\n",
      "Epoch 117/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7291/7291 [==============================] - 1s 83us/sample - loss: 0.0292 - acc: 0.9919 - val_loss: 0.4543 - val_acc: 0.9178\n",
      "Epoch 118/300\n",
      "7291/7291 [==============================] - 1s 84us/sample - loss: 0.0224 - acc: 0.9945 - val_loss: 0.4663 - val_acc: 0.9143\n",
      "Epoch 119/300\n",
      "7291/7291 [==============================] - 1s 85us/sample - loss: 0.0467 - acc: 0.9852 - val_loss: 0.4694 - val_acc: 0.9048\n",
      "Epoch 120/300\n",
      "7291/7291 [==============================] - 1s 85us/sample - loss: 0.0542 - acc: 0.9834 - val_loss: 0.4369 - val_acc: 0.9118\n",
      "Epoch 121/300\n",
      "7291/7291 [==============================] - 1s 87us/sample - loss: 0.0477 - acc: 0.9848 - val_loss: 0.3828 - val_acc: 0.9188\n",
      "Epoch 122/300\n",
      "7291/7291 [==============================] - 1s 85us/sample - loss: 0.0528 - acc: 0.9827 - val_loss: 0.3884 - val_acc: 0.9173\n",
      "Epoch 123/300\n",
      "7291/7291 [==============================] - 1s 87us/sample - loss: 0.0490 - acc: 0.9823 - val_loss: 0.4114 - val_acc: 0.9163\n",
      "Epoch 124/300\n",
      "7291/7291 [==============================] - 1s 86us/sample - loss: 0.0334 - acc: 0.9903 - val_loss: 0.4688 - val_acc: 0.9143\n",
      "Epoch 125/300\n",
      "7291/7291 [==============================] - 1s 86us/sample - loss: 0.0378 - acc: 0.9871 - val_loss: 0.4364 - val_acc: 0.9168\n",
      "Epoch 126/300\n",
      "7291/7291 [==============================] - 1s 92us/sample - loss: 0.0247 - acc: 0.9938 - val_loss: 0.4225 - val_acc: 0.9188\n",
      "Epoch 127/300\n",
      "7291/7291 [==============================] - 1s 86us/sample - loss: 0.0191 - acc: 0.9944 - val_loss: 0.4536 - val_acc: 0.9163\n",
      "Epoch 128/300\n",
      "7291/7291 [==============================] - 1s 78us/sample - loss: 0.0224 - acc: 0.9926 - val_loss: 0.4503 - val_acc: 0.9168\n",
      "Epoch 129/300\n",
      "7291/7291 [==============================] - 1s 90us/sample - loss: 0.0323 - acc: 0.9885 - val_loss: 0.4680 - val_acc: 0.9098\n",
      "Epoch 130/300\n",
      "7291/7291 [==============================] - 1s 85us/sample - loss: 0.0556 - acc: 0.9794 - val_loss: 0.4465 - val_acc: 0.9113\n",
      "Epoch 131/300\n",
      "7291/7291 [==============================] - 1s 89us/sample - loss: 0.0371 - acc: 0.9878 - val_loss: 0.4831 - val_acc: 0.9128\n",
      "Epoch 132/300\n",
      "7291/7291 [==============================] - 1s 109us/sample - loss: 0.0277 - acc: 0.9916 - val_loss: 0.4795 - val_acc: 0.9118\n",
      "Epoch 133/300\n",
      "7291/7291 [==============================] - 1s 100us/sample - loss: 0.0253 - acc: 0.9918 - val_loss: 0.4454 - val_acc: 0.9148\n",
      "Epoch 134/300\n",
      "7291/7291 [==============================] - 1s 92us/sample - loss: 0.0306 - acc: 0.9896 - val_loss: 0.4730 - val_acc: 0.9108\n",
      "Epoch 135/300\n",
      "7291/7291 [==============================] - 1s 89us/sample - loss: 0.0341 - acc: 0.9896 - val_loss: 0.4427 - val_acc: 0.9093\n",
      "Epoch 136/300\n",
      "7291/7291 [==============================] - 1s 85us/sample - loss: 0.0381 - acc: 0.9875 - val_loss: 0.4501 - val_acc: 0.9113\n",
      "Epoch 137/300\n",
      "7291/7291 [==============================] - 1s 88us/sample - loss: 0.0365 - acc: 0.9879 - val_loss: 0.4618 - val_acc: 0.9098\n",
      "Epoch 138/300\n",
      "7291/7291 [==============================] - 1s 84us/sample - loss: 0.0534 - acc: 0.9831 - val_loss: 0.4608 - val_acc: 0.9013\n",
      "Epoch 139/300\n",
      "7291/7291 [==============================] - 1s 108us/sample - loss: 0.0732 - acc: 0.9748 - val_loss: 0.4735 - val_acc: 0.9113\n",
      "Epoch 140/300\n",
      "7291/7291 [==============================] - 1s 188us/sample - loss: 0.0596 - acc: 0.9790 - val_loss: 0.5364 - val_acc: 0.9068\n",
      "Epoch 141/300\n",
      "7291/7291 [==============================] - 1s 115us/sample - loss: 0.0553 - acc: 0.9812 - val_loss: 0.4761 - val_acc: 0.9078\n",
      "Epoch 142/300\n",
      "7291/7291 [==============================] - 1s 119us/sample - loss: 0.0627 - acc: 0.9802 - val_loss: 0.4924 - val_acc: 0.9098\n",
      "Epoch 143/300\n",
      "7291/7291 [==============================] - 1s 106us/sample - loss: 0.0622 - acc: 0.9809 - val_loss: 0.4521 - val_acc: 0.9098\n",
      "Epoch 144/300\n",
      "7291/7291 [==============================] - 1s 120us/sample - loss: 0.0489 - acc: 0.9833 - val_loss: 0.4497 - val_acc: 0.9053\n",
      "Epoch 145/300\n",
      "7291/7291 [==============================] - 1s 119us/sample - loss: 0.0334 - acc: 0.9892 - val_loss: 0.4401 - val_acc: 0.9208\n",
      "Epoch 146/300\n",
      "7291/7291 [==============================] - 1s 112us/sample - loss: 0.0483 - acc: 0.9835 - val_loss: 0.4310 - val_acc: 0.9133\n",
      "Epoch 147/300\n",
      "7291/7291 [==============================] - 1s 110us/sample - loss: 0.0335 - acc: 0.9882 - val_loss: 0.4379 - val_acc: 0.9208\n",
      "Epoch 148/300\n",
      "7291/7291 [==============================] - 1s 128us/sample - loss: 0.0292 - acc: 0.9907 - val_loss: 0.4415 - val_acc: 0.9208\n",
      "Epoch 149/300\n",
      "7291/7291 [==============================] - 1s 149us/sample - loss: 0.0343 - acc: 0.9890 - val_loss: 0.4683 - val_acc: 0.9153\n",
      "Epoch 150/300\n",
      "7291/7291 [==============================] - 1s 183us/sample - loss: 0.0340 - acc: 0.9877 - val_loss: 0.4422 - val_acc: 0.9153\n",
      "Epoch 151/300\n",
      "7291/7291 [==============================] - 1s 148us/sample - loss: 0.0406 - acc: 0.9871 - val_loss: 0.4660 - val_acc: 0.9048\n",
      "Epoch 152/300\n",
      "7291/7291 [==============================] - 1s 107us/sample - loss: 0.0362 - acc: 0.9871 - val_loss: 0.4676 - val_acc: 0.9108\n",
      "Epoch 153/300\n",
      "7291/7291 [==============================] - 1s 195us/sample - loss: 0.0580 - acc: 0.9826 - val_loss: 0.5087 - val_acc: 0.9038\n",
      "Epoch 154/300\n",
      "7291/7291 [==============================] - 1s 185us/sample - loss: 0.0795 - acc: 0.9737 - val_loss: 0.4526 - val_acc: 0.9178\n",
      "Epoch 155/300\n",
      "7291/7291 [==============================] - 1s 112us/sample - loss: 0.0547 - acc: 0.9812 - val_loss: 0.4606 - val_acc: 0.9098\n",
      "Epoch 156/300\n",
      "7291/7291 [==============================] - 1s 142us/sample - loss: 0.0412 - acc: 0.9863 - val_loss: 0.4870 - val_acc: 0.9143\n",
      "Epoch 157/300\n",
      "7291/7291 [==============================] - 1s 129us/sample - loss: 0.0322 - acc: 0.9899 - val_loss: 0.4885 - val_acc: 0.9138\n",
      "Epoch 158/300\n",
      "7291/7291 [==============================] - 1s 171us/sample - loss: 0.0248 - acc: 0.9930 - val_loss: 0.4448 - val_acc: 0.9163\n",
      "Epoch 159/300\n",
      "7291/7291 [==============================] - 1s 120us/sample - loss: 0.0211 - acc: 0.9937 - val_loss: 0.4637 - val_acc: 0.9143\n",
      "Epoch 160/300\n",
      "7291/7291 [==============================] - 1s 115us/sample - loss: 0.0179 - acc: 0.9953 - val_loss: 0.4654 - val_acc: 0.9183\n",
      "Epoch 161/300\n",
      "7291/7291 [==============================] - 1s 113us/sample - loss: 0.0209 - acc: 0.9936 - val_loss: 0.4850 - val_acc: 0.9128\n",
      "Epoch 162/300\n",
      "7291/7291 [==============================] - 1s 105us/sample - loss: 0.0348 - acc: 0.9896 - val_loss: 0.4671 - val_acc: 0.9178\n",
      "Epoch 163/300\n",
      "7291/7291 [==============================] - 1s 103us/sample - loss: 0.0295 - acc: 0.9909 - val_loss: 0.4240 - val_acc: 0.9183\n",
      "Epoch 164/300\n",
      "7291/7291 [==============================] - 1s 100us/sample - loss: 0.0305 - acc: 0.9900 - val_loss: 0.4085 - val_acc: 0.9223\n",
      "Epoch 165/300\n",
      "7291/7291 [==============================] - 1s 121us/sample - loss: 0.0239 - acc: 0.9938 - val_loss: 0.4879 - val_acc: 0.9123\n",
      "Epoch 166/300\n",
      "7291/7291 [==============================] - 1s 127us/sample - loss: 0.0245 - acc: 0.9925 - val_loss: 0.4731 - val_acc: 0.9168\n",
      "Epoch 167/300\n",
      "7291/7291 [==============================] - 1s 118us/sample - loss: 0.0239 - acc: 0.9934 - val_loss: 0.4497 - val_acc: 0.9198\n",
      "Epoch 168/300\n",
      "7291/7291 [==============================] - 1s 107us/sample - loss: 0.0307 - acc: 0.9896 - val_loss: 0.4939 - val_acc: 0.9163\n",
      "Epoch 169/300\n",
      "7291/7291 [==============================] - 1s 113us/sample - loss: 0.0418 - acc: 0.9879 - val_loss: 0.4754 - val_acc: 0.9153\n",
      "Epoch 170/300\n",
      "7291/7291 [==============================] - 1s 111us/sample - loss: 0.0289 - acc: 0.9909 - val_loss: 0.4804 - val_acc: 0.9128\n",
      "Epoch 171/300\n",
      "7291/7291 [==============================] - 1s 145us/sample - loss: 0.0445 - acc: 0.9851 - val_loss: 0.5098 - val_acc: 0.9063\n",
      "Epoch 172/300\n",
      "7291/7291 [==============================] - 1s 146us/sample - loss: 0.0495 - acc: 0.9841 - val_loss: 0.4639 - val_acc: 0.9108\n",
      "Epoch 173/300\n",
      "7291/7291 [==============================] - 2s 242us/sample - loss: 0.0406 - acc: 0.9864 - val_loss: 0.4570 - val_acc: 0.9113\n",
      "Epoch 174/300\n",
      "7291/7291 [==============================] - 2s 264us/sample - loss: 0.0369 - acc: 0.9867 - val_loss: 0.4939 - val_acc: 0.9078\n",
      "Epoch 175/300\n",
      "7291/7291 [==============================] - 2s 332us/sample - loss: 0.0268 - acc: 0.9923 - val_loss: 0.5083 - val_acc: 0.9123\n",
      "Epoch 176/300\n",
      "7291/7291 [==============================] - 2s 209us/sample - loss: 0.0274 - acc: 0.9912 - val_loss: 0.4963 - val_acc: 0.9098\n",
      "Epoch 177/300\n",
      "7291/7291 [==============================] - 1s 166us/sample - loss: 0.0342 - acc: 0.9907 - val_loss: 0.4598 - val_acc: 0.9178\n",
      "Epoch 178/300\n",
      "7291/7291 [==============================] - 1s 169us/sample - loss: 0.0431 - acc: 0.9860 - val_loss: 0.4706 - val_acc: 0.9178\n",
      "Epoch 179/300\n",
      "7291/7291 [==============================] - 1s 148us/sample - loss: 0.0353 - acc: 0.9886 - val_loss: 0.4936 - val_acc: 0.9088\n",
      "Epoch 180/300\n",
      "7291/7291 [==============================] - 2s 226us/sample - loss: 0.0201 - acc: 0.9941 - val_loss: 0.4918 - val_acc: 0.9168\n",
      "Epoch 181/300\n",
      "7291/7291 [==============================] - 2s 211us/sample - loss: 0.0188 - acc: 0.9947 - val_loss: 0.4803 - val_acc: 0.9128\n",
      "Epoch 182/300\n",
      "7291/7291 [==============================] - 1s 174us/sample - loss: 0.0177 - acc: 0.9953 - val_loss: 0.4878 - val_acc: 0.9138\n",
      "Epoch 183/300\n",
      "7291/7291 [==============================] - 1s 175us/sample - loss: 0.0154 - acc: 0.9955 - val_loss: 0.4807 - val_acc: 0.9168\n",
      "Epoch 184/300\n",
      "7291/7291 [==============================] - 1s 147us/sample - loss: 0.0307 - acc: 0.9903 - val_loss: 0.4834 - val_acc: 0.9153\n",
      "Epoch 185/300\n",
      "7291/7291 [==============================] - 1s 177us/sample - loss: 0.0336 - acc: 0.9894 - val_loss: 0.4800 - val_acc: 0.9103\n",
      "Epoch 186/300\n",
      "7291/7291 [==============================] - 1s 188us/sample - loss: 0.0529 - acc: 0.9830 - val_loss: 0.4701 - val_acc: 0.9103\n",
      "Epoch 187/300\n",
      "7291/7291 [==============================] - 1s 164us/sample - loss: 0.0460 - acc: 0.9840 - val_loss: 0.4882 - val_acc: 0.9088\n",
      "Epoch 188/300\n",
      "7291/7291 [==============================] - 1s 142us/sample - loss: 0.0573 - acc: 0.9798 - val_loss: 0.5099 - val_acc: 0.9013\n",
      "Epoch 189/300\n",
      "7291/7291 [==============================] - 1s 174us/sample - loss: 0.0535 - acc: 0.9818 - val_loss: 0.4465 - val_acc: 0.9078\n",
      "Epoch 190/300\n",
      "7291/7291 [==============================] - 2s 222us/sample - loss: 0.0357 - acc: 0.9867 - val_loss: 0.4746 - val_acc: 0.9113\n",
      "Epoch 191/300\n",
      "7291/7291 [==============================] - 1s 192us/sample - loss: 0.0270 - acc: 0.9918 - val_loss: 0.4553 - val_acc: 0.9178\n",
      "Epoch 192/300\n",
      "7291/7291 [==============================] - 1s 186us/sample - loss: 0.0225 - acc: 0.9941 - val_loss: 0.4279 - val_acc: 0.9153\n",
      "Epoch 193/300\n",
      "7291/7291 [==============================] - 1s 177us/sample - loss: 0.0219 - acc: 0.9947 - val_loss: 0.4048 - val_acc: 0.9208\n",
      "Epoch 194/300\n",
      "7291/7291 [==============================] - 1s 130us/sample - loss: 0.0287 - acc: 0.9907 - val_loss: 0.4703 - val_acc: 0.9178\n",
      "Epoch 195/300\n",
      "7291/7291 [==============================] - 1s 120us/sample - loss: 0.0286 - acc: 0.9903 - val_loss: 0.4341 - val_acc: 0.9128\n",
      "Epoch 196/300\n",
      "7291/7291 [==============================] - 1s 160us/sample - loss: 0.0249 - acc: 0.9918 - val_loss: 0.4635 - val_acc: 0.9133\n",
      "Epoch 197/300\n",
      "7291/7291 [==============================] - 1s 182us/sample - loss: 0.0295 - acc: 0.9890 - val_loss: 0.4774 - val_acc: 0.9138\n",
      "Epoch 198/300\n",
      "7291/7291 [==============================] - 1s 198us/sample - loss: 0.0491 - acc: 0.9830 - val_loss: 0.4817 - val_acc: 0.9043\n",
      "Epoch 199/300\n",
      "7291/7291 [==============================] - 1s 185us/sample - loss: 0.0491 - acc: 0.9856 - val_loss: 0.4525 - val_acc: 0.9188\n",
      "Epoch 200/300\n",
      "7291/7291 [==============================] - 1s 144us/sample - loss: 0.0311 - acc: 0.9907 - val_loss: 0.5041 - val_acc: 0.9093\n",
      "Epoch 201/300\n",
      "7291/7291 [==============================] - 1s 130us/sample - loss: 0.0360 - acc: 0.9866 - val_loss: 0.4738 - val_acc: 0.9098\n",
      "Epoch 202/300\n",
      "7291/7291 [==============================] - 1s 130us/sample - loss: 0.0262 - acc: 0.9927 - val_loss: 0.4816 - val_acc: 0.9103\n",
      "Epoch 203/300\n",
      "7291/7291 [==============================] - 1s 164us/sample - loss: 0.0213 - acc: 0.9940 - val_loss: 0.4863 - val_acc: 0.9063\n",
      "Epoch 204/300\n",
      "7291/7291 [==============================] - 1s 189us/sample - loss: 0.0139 - acc: 0.9967 - val_loss: 0.4600 - val_acc: 0.9148\n",
      "Epoch 205/300\n",
      "7291/7291 [==============================] - 1s 161us/sample - loss: 0.0118 - acc: 0.9970 - val_loss: 0.4572 - val_acc: 0.9188\n",
      "Epoch 206/300\n",
      "7291/7291 [==============================] - 1s 166us/sample - loss: 0.0112 - acc: 0.9979 - val_loss: 0.4871 - val_acc: 0.9133\n",
      "Epoch 207/300\n",
      "7291/7291 [==============================] - 1s 161us/sample - loss: 0.0085 - acc: 0.9986 - val_loss: 0.4700 - val_acc: 0.9233\n",
      "Epoch 208/300\n",
      "7291/7291 [==============================] - 1s 187us/sample - loss: 0.0435 - acc: 0.9863 - val_loss: 0.5834 - val_acc: 0.9028\n",
      "Epoch 209/300\n",
      "7291/7291 [==============================] - 1s 172us/sample - loss: 0.0895 - acc: 0.9734 - val_loss: 0.5039 - val_acc: 0.9028\n",
      "Epoch 210/300\n",
      "7291/7291 [==============================] - 1s 147us/sample - loss: 0.0519 - acc: 0.9822 - val_loss: 0.5636 - val_acc: 0.8984\n",
      "Epoch 211/300\n",
      "7291/7291 [==============================] - 1s 141us/sample - loss: 0.0524 - acc: 0.9826 - val_loss: 0.5251 - val_acc: 0.9068\n",
      "Epoch 212/300\n",
      "7291/7291 [==============================] - 1s 178us/sample - loss: 0.0349 - acc: 0.9890 - val_loss: 0.4582 - val_acc: 0.9168\n",
      "Epoch 213/300\n",
      "7291/7291 [==============================] - 1s 171us/sample - loss: 0.0274 - acc: 0.9920 - val_loss: 0.4898 - val_acc: 0.9103\n",
      "Epoch 214/300\n",
      "7291/7291 [==============================] - 1s 158us/sample - loss: 0.0344 - acc: 0.9889 - val_loss: 0.4918 - val_acc: 0.9113\n",
      "Epoch 215/300\n",
      "7291/7291 [==============================] - 1s 201us/sample - loss: 0.0268 - acc: 0.9912 - val_loss: 0.4592 - val_acc: 0.9173\n",
      "Epoch 216/300\n",
      "7291/7291 [==============================] - 1s 139us/sample - loss: 0.0190 - acc: 0.9949 - val_loss: 0.5141 - val_acc: 0.9093\n",
      "Epoch 217/300\n",
      "7291/7291 [==============================] - 1s 118us/sample - loss: 0.0126 - acc: 0.9971 - val_loss: 0.4736 - val_acc: 0.9133\n",
      "Epoch 218/300\n",
      "7291/7291 [==============================] - 1s 117us/sample - loss: 0.0222 - acc: 0.9929 - val_loss: 0.4926 - val_acc: 0.9088\n",
      "Epoch 219/300\n",
      "7291/7291 [==============================] - 1s 118us/sample - loss: 0.0270 - acc: 0.9927 - val_loss: 0.4970 - val_acc: 0.9108\n",
      "Epoch 220/300\n",
      "7291/7291 [==============================] - 1s 104us/sample - loss: 0.0308 - acc: 0.9897 - val_loss: 0.4502 - val_acc: 0.9153\n",
      "Epoch 221/300\n",
      "7291/7291 [==============================] - 1s 97us/sample - loss: 0.0211 - acc: 0.9936 - val_loss: 0.4985 - val_acc: 0.9123\n",
      "Epoch 222/300\n",
      "7291/7291 [==============================] - 1s 112us/sample - loss: 0.0281 - acc: 0.9907 - val_loss: 0.5024 - val_acc: 0.9113\n",
      "Epoch 223/300\n",
      "7291/7291 [==============================] - 1s 109us/sample - loss: 0.0135 - acc: 0.9962 - val_loss: 0.4913 - val_acc: 0.9213\n",
      "Epoch 224/300\n",
      "7291/7291 [==============================] - 1s 122us/sample - loss: 0.0092 - acc: 0.9982 - val_loss: 0.4764 - val_acc: 0.9178\n",
      "Epoch 225/300\n",
      "7291/7291 [==============================] - 1s 133us/sample - loss: 0.0071 - acc: 0.9995 - val_loss: 0.4651 - val_acc: 0.9163\n",
      "Epoch 226/300\n",
      "7291/7291 [==============================] - 1s 126us/sample - loss: 0.0182 - acc: 0.9953 - val_loss: 0.5550 - val_acc: 0.9048\n",
      "Epoch 227/300\n",
      "7291/7291 [==============================] - 1s 97us/sample - loss: 0.0738 - acc: 0.9770 - val_loss: 0.5181 - val_acc: 0.9023\n",
      "Epoch 228/300\n",
      "7291/7291 [==============================] - 1s 113us/sample - loss: 0.0579 - acc: 0.9798 - val_loss: 0.4638 - val_acc: 0.9173\n",
      "Epoch 229/300\n",
      "7291/7291 [==============================] - 1s 95us/sample - loss: 0.0285 - acc: 0.9904 - val_loss: 0.4638 - val_acc: 0.9148\n",
      "Epoch 230/300\n",
      "7291/7291 [==============================] - 1s 93us/sample - loss: 0.0275 - acc: 0.9915 - val_loss: 0.4605 - val_acc: 0.9198\n",
      "Epoch 231/300\n",
      "7291/7291 [==============================] - 1s 98us/sample - loss: 0.0163 - acc: 0.9957 - val_loss: 0.4747 - val_acc: 0.9158\n",
      "Epoch 232/300\n",
      "7291/7291 [==============================] - 1s 122us/sample - loss: 0.0160 - acc: 0.9949 - val_loss: 0.4804 - val_acc: 0.9183\n",
      "Epoch 233/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7291/7291 [==============================] - 1s 146us/sample - loss: 0.0113 - acc: 0.9977 - val_loss: 0.5045 - val_acc: 0.9188\n",
      "Epoch 234/300\n",
      "7291/7291 [==============================] - 1s 171us/sample - loss: 0.0065 - acc: 0.9990 - val_loss: 0.4588 - val_acc: 0.9243\n",
      "Epoch 235/300\n",
      "7291/7291 [==============================] - 1s 150us/sample - loss: 0.0077 - acc: 0.9981 - val_loss: 0.5029 - val_acc: 0.9173\n",
      "Epoch 236/300\n",
      "7291/7291 [==============================] - 1s 125us/sample - loss: 0.0247 - acc: 0.9937 - val_loss: 0.5188 - val_acc: 0.9118\n",
      "Epoch 237/300\n",
      "7291/7291 [==============================] - 1s 115us/sample - loss: 0.0426 - acc: 0.9872 - val_loss: 0.5386 - val_acc: 0.9133\n",
      "Epoch 238/300\n",
      "7291/7291 [==============================] - 1s 99us/sample - loss: 0.0453 - acc: 0.9864 - val_loss: 0.4845 - val_acc: 0.9143\n",
      "Epoch 239/300\n",
      "7291/7291 [==============================] - 1s 115us/sample - loss: 0.0535 - acc: 0.9834 - val_loss: 0.4853 - val_acc: 0.9168\n",
      "Epoch 240/300\n",
      "7291/7291 [==============================] - 1s 110us/sample - loss: 0.0507 - acc: 0.9833 - val_loss: 0.4372 - val_acc: 0.9128\n",
      "Epoch 241/300\n",
      "7291/7291 [==============================] - 1s 137us/sample - loss: 0.0322 - acc: 0.9883 - val_loss: 0.4713 - val_acc: 0.9138\n",
      "Epoch 242/300\n",
      "7291/7291 [==============================] - 1s 130us/sample - loss: 0.0243 - acc: 0.9927 - val_loss: 0.4385 - val_acc: 0.9218\n",
      "Epoch 243/300\n",
      "7291/7291 [==============================] - 1s 120us/sample - loss: 0.0326 - acc: 0.9911 - val_loss: 0.4731 - val_acc: 0.9108\n",
      "Epoch 244/300\n",
      "7291/7291 [==============================] - 1s 136us/sample - loss: 0.0359 - acc: 0.9883 - val_loss: 0.4536 - val_acc: 0.9148\n",
      "Epoch 245/300\n",
      "7291/7291 [==============================] - 1s 156us/sample - loss: 0.0285 - acc: 0.9901 - val_loss: 0.4594 - val_acc: 0.9188\n",
      "Epoch 246/300\n",
      "7291/7291 [==============================] - 1s 110us/sample - loss: 0.0212 - acc: 0.9938 - val_loss: 0.4732 - val_acc: 0.9218\n",
      "Epoch 247/300\n",
      "7291/7291 [==============================] - 1s 107us/sample - loss: 0.0333 - acc: 0.9908 - val_loss: 0.4768 - val_acc: 0.9203\n",
      "Epoch 248/300\n",
      "7291/7291 [==============================] - 1s 120us/sample - loss: 0.0212 - acc: 0.9933 - val_loss: 0.4806 - val_acc: 0.9163\n",
      "Epoch 249/300\n",
      "7291/7291 [==============================] - 1s 111us/sample - loss: 0.0244 - acc: 0.9925 - val_loss: 0.4652 - val_acc: 0.9193\n",
      "Epoch 250/300\n",
      "7291/7291 [==============================] - 1s 123us/sample - loss: 0.0205 - acc: 0.9944 - val_loss: 0.4974 - val_acc: 0.9178\n",
      "Epoch 251/300\n",
      "7291/7291 [==============================] - 1s 118us/sample - loss: 0.0173 - acc: 0.9957 - val_loss: 0.4708 - val_acc: 0.9208\n",
      "Epoch 252/300\n",
      "7291/7291 [==============================] - 1s 142us/sample - loss: 0.0149 - acc: 0.9957 - val_loss: 0.4596 - val_acc: 0.9233\n",
      "Epoch 253/300\n",
      "7291/7291 [==============================] - 1s 166us/sample - loss: 0.0145 - acc: 0.9960 - val_loss: 0.5130 - val_acc: 0.9218\n",
      "Epoch 254/300\n",
      "7291/7291 [==============================] - 1s 158us/sample - loss: 0.0171 - acc: 0.9951 - val_loss: 0.4926 - val_acc: 0.9228\n",
      "Epoch 255/300\n",
      "7291/7291 [==============================] - 1s 175us/sample - loss: 0.0296 - acc: 0.9901 - val_loss: 0.5237 - val_acc: 0.9133\n",
      "Epoch 256/300\n",
      "7291/7291 [==============================] - 1s 177us/sample - loss: 0.0281 - acc: 0.9908 - val_loss: 0.5071 - val_acc: 0.9168\n",
      "Epoch 257/300\n",
      "7291/7291 [==============================] - 1s 143us/sample - loss: 0.0259 - acc: 0.9914 - val_loss: 0.4889 - val_acc: 0.9168\n",
      "Epoch 258/300\n",
      "7291/7291 [==============================] - 1s 161us/sample - loss: 0.0187 - acc: 0.9945 - val_loss: 0.5468 - val_acc: 0.9133\n",
      "Epoch 259/300\n",
      "7291/7291 [==============================] - 1s 201us/sample - loss: 0.0512 - acc: 0.9844 - val_loss: 0.5607 - val_acc: 0.9158\n",
      "Epoch 260/300\n",
      "7291/7291 [==============================] - 1s 158us/sample - loss: 0.0637 - acc: 0.9793 - val_loss: 0.5344 - val_acc: 0.9148\n",
      "Epoch 261/300\n",
      "7291/7291 [==============================] - 1s 175us/sample - loss: 0.0268 - acc: 0.9904 - val_loss: 0.4809 - val_acc: 0.9103\n",
      "Epoch 262/300\n",
      "7291/7291 [==============================] - 1s 166us/sample - loss: 0.0258 - acc: 0.9925 - val_loss: 0.4858 - val_acc: 0.9178\n",
      "Epoch 263/300\n",
      "7291/7291 [==============================] - 1s 121us/sample - loss: 0.0173 - acc: 0.9947 - val_loss: 0.5031 - val_acc: 0.9168\n",
      "Epoch 264/300\n",
      "7291/7291 [==============================] - 1s 133us/sample - loss: 0.0187 - acc: 0.9953 - val_loss: 0.4781 - val_acc: 0.9198\n",
      "Epoch 265/300\n",
      "7291/7291 [==============================] - 1s 141us/sample - loss: 0.0144 - acc: 0.9959 - val_loss: 0.5230 - val_acc: 0.9173\n",
      "Epoch 266/300\n",
      "7291/7291 [==============================] - 1s 117us/sample - loss: 0.0320 - acc: 0.9904 - val_loss: 0.5015 - val_acc: 0.9093\n",
      "Epoch 267/300\n",
      "7291/7291 [==============================] - 1s 102us/sample - loss: 0.0415 - acc: 0.9866 - val_loss: 0.5156 - val_acc: 0.9118\n",
      "Epoch 268/300\n",
      "7291/7291 [==============================] - 1s 99us/sample - loss: 0.0210 - acc: 0.9931 - val_loss: 0.5037 - val_acc: 0.9148\n",
      "Epoch 269/300\n",
      "7291/7291 [==============================] - 1s 98us/sample - loss: 0.0207 - acc: 0.9933 - val_loss: 0.5117 - val_acc: 0.9173\n",
      "Epoch 270/300\n",
      "7291/7291 [==============================] - 1s 144us/sample - loss: 0.0179 - acc: 0.9949 - val_loss: 0.5422 - val_acc: 0.9123\n",
      "Epoch 271/300\n",
      "7291/7291 [==============================] - 1s 172us/sample - loss: 0.0213 - acc: 0.9918 - val_loss: 0.5190 - val_acc: 0.9118\n",
      "Epoch 272/300\n",
      "7291/7291 [==============================] - 1s 156us/sample - loss: 0.0208 - acc: 0.9931 - val_loss: 0.5393 - val_acc: 0.9163\n",
      "Epoch 273/300\n",
      "7291/7291 [==============================] - 1s 131us/sample - loss: 0.0208 - acc: 0.9930 - val_loss: 0.5431 - val_acc: 0.9193\n",
      "Epoch 274/300\n",
      "7291/7291 [==============================] - 1s 149us/sample - loss: 0.0281 - acc: 0.9923 - val_loss: 0.5939 - val_acc: 0.9073\n",
      "Epoch 275/300\n",
      "7291/7291 [==============================] - 1s 155us/sample - loss: 0.0433 - acc: 0.9842 - val_loss: 0.5494 - val_acc: 0.9123\n",
      "Epoch 276/300\n",
      "7291/7291 [==============================] - 1s 182us/sample - loss: 0.0461 - acc: 0.9872 - val_loss: 0.5515 - val_acc: 0.8994\n",
      "Epoch 277/300\n",
      "7291/7291 [==============================] - 2s 218us/sample - loss: 0.0543 - acc: 0.9848 - val_loss: 0.4947 - val_acc: 0.9078\n",
      "Epoch 278/300\n",
      "7291/7291 [==============================] - 1s 171us/sample - loss: 0.0579 - acc: 0.9815 - val_loss: 0.5107 - val_acc: 0.9068\n",
      "Epoch 279/300\n",
      "7291/7291 [==============================] - 1s 184us/sample - loss: 0.0417 - acc: 0.9875 - val_loss: 0.5157 - val_acc: 0.9053\n",
      "Epoch 280/300\n",
      "7291/7291 [==============================] - 1s 181us/sample - loss: 0.0292 - acc: 0.9912 - val_loss: 0.4983 - val_acc: 0.9168\n",
      "Epoch 281/300\n",
      "7291/7291 [==============================] - 1s 163us/sample - loss: 0.0247 - acc: 0.9930 - val_loss: 0.5266 - val_acc: 0.9188\n",
      "Epoch 282/300\n",
      "7291/7291 [==============================] - 1s 164us/sample - loss: 0.0248 - acc: 0.9929 - val_loss: 0.5469 - val_acc: 0.9158\n",
      "Epoch 283/300\n",
      "7291/7291 [==============================] - 1s 164us/sample - loss: 0.0268 - acc: 0.9920 - val_loss: 0.5275 - val_acc: 0.9133\n",
      "Epoch 284/300\n",
      "7291/7291 [==============================] - 1s 192us/sample - loss: 0.0218 - acc: 0.9931 - val_loss: 0.4946 - val_acc: 0.9128\n",
      "Epoch 285/300\n",
      "7291/7291 [==============================] - 1s 169us/sample - loss: 0.0165 - acc: 0.9955 - val_loss: 0.5273 - val_acc: 0.9133\n",
      "Epoch 286/300\n",
      "7291/7291 [==============================] - 1s 140us/sample - loss: 0.0132 - acc: 0.9967 - val_loss: 0.4952 - val_acc: 0.9103\n",
      "Epoch 287/300\n",
      "7291/7291 [==============================] - 1s 115us/sample - loss: 0.0110 - acc: 0.9974 - val_loss: 0.4862 - val_acc: 0.9168\n",
      "Epoch 288/300\n",
      "7291/7291 [==============================] - 1s 104us/sample - loss: 0.0091 - acc: 0.9985 - val_loss: 0.5221 - val_acc: 0.9143\n",
      "Epoch 289/300\n",
      "7291/7291 [==============================] - 1s 97us/sample - loss: 0.0195 - acc: 0.9947 - val_loss: 0.4814 - val_acc: 0.9173\n",
      "Epoch 290/300\n",
      "7291/7291 [==============================] - 1s 94us/sample - loss: 0.0213 - acc: 0.9929 - val_loss: 0.5062 - val_acc: 0.9178\n",
      "Epoch 291/300\n",
      "7291/7291 [==============================] - 1s 92us/sample - loss: 0.0194 - acc: 0.9945 - val_loss: 0.5009 - val_acc: 0.9158\n",
      "Epoch 292/300\n",
      "7291/7291 [==============================] - 1s 90us/sample - loss: 0.0109 - acc: 0.9967 - val_loss: 0.5260 - val_acc: 0.9153\n",
      "Epoch 293/300\n",
      "7291/7291 [==============================] - 1s 106us/sample - loss: 0.0530 - acc: 0.9830 - val_loss: 0.5078 - val_acc: 0.9108\n",
      "Epoch 294/300\n",
      "7291/7291 [==============================] - 1s 130us/sample - loss: 0.0584 - acc: 0.9827 - val_loss: 0.5121 - val_acc: 0.8984\n",
      "Epoch 295/300\n",
      "7291/7291 [==============================] - 1s 113us/sample - loss: 0.0496 - acc: 0.9846 - val_loss: 0.4278 - val_acc: 0.9213\n",
      "Epoch 296/300\n",
      "7291/7291 [==============================] - 1s 188us/sample - loss: 0.0229 - acc: 0.9936 - val_loss: 0.4710 - val_acc: 0.9173\n",
      "Epoch 297/300\n",
      "7291/7291 [==============================] - 1s 183us/sample - loss: 0.0128 - acc: 0.9968 - val_loss: 0.4595 - val_acc: 0.9188\n",
      "Epoch 298/300\n",
      "7291/7291 [==============================] - 1s 119us/sample - loss: 0.0112 - acc: 0.9977 - val_loss: 0.4622 - val_acc: 0.9208\n",
      "Epoch 299/300\n",
      "7291/7291 [==============================] - 1s 105us/sample - loss: 0.0076 - acc: 0.9986 - val_loss: 0.4882 - val_acc: 0.9198\n",
      "Epoch 300/300\n",
      "7291/7291 [==============================] - 1s 97us/sample - loss: 0.0180 - acc: 0.9949 - val_loss: 0.4931 - val_acc: 0.9188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1320fa668>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_ = np.reshape(Xtrain, [-1, int(dataDimension / inputDims), inputDims])\n",
    "Xtest_ = np.reshape(Xtest, [-1, int(dataDimension / inputDims), inputDims])\n",
    "model.fit(Xtrain_, Ytrain, batchSize, epochs=totalEpochs, validation_data=(Xtest_, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "fastcell_example_keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
